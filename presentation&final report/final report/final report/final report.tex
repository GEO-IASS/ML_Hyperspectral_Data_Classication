%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{article}
%\input{mydef.tex}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{amssymb,amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{url}
\usepackage[stable]{footmisc}
\usepackage{booktabs}
\usepackage[square]{natbib}
\usepackage{indentfirst}
\usepackage[colorlinks, linkcolor=red, anchorcolor=purple, citecolor=blue]{hyperref}
\usepackage{hyperref}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{multicol}
\usepackage{caption}
\setlength{\columnsep}{1cm}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\setlength{\headheight}{13.6pt}
\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{CS 57800: Statistical Machine Learning} % Top left header
\chead{}
\rhead{Project Proposal} % Top right header
\lfoot{} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\setlength{\parskip}{.2\baselineskip}
%\setlength\parindent{0pt} % Removes all indentation from paragraphs

\title{
\textbf{Final Report} \\ \textsc{\textit{Hyperspectral Data Classification}} \\
}

\author{
	\textbf{\textit{Jun Chen, Dan Wang, Zhou Zhang and  Penghao Wang}} \\
	Purdue University\\
	\texttt{chen1241@purdue.edu, wang1766@purdue.edu}\\
	\texttt{zhan1553@purdue.edu, wang1128@purdue.edu}
}

\date{\today}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%\thispagestyle{empty}

\section{Background and Motivation}

Conservation tillage management has been advocated for the purpose of soil preservation and sustainable crop production. Conservational tillage practice induces less surface disturbance and leaves more crop residues, which can decrease runoff rate, improve soil and water quality, and increase organic matter. The demand for the mapping of crop tillage practices has been brought up for precision agricultural management and appraisal. However, current methods for mapping massive crop tillage practices are mainly done with field investigations, which are labor costing, time consuming, subjective, and make it difficult to generate widely distributed survey data. Remote sensing technology provides a more rapid, accurate, and objective solution. Moreover, the vast data from remote sensing in agriculture require more efficient approaches in data analytics, including tillage mapping, in support of management decisions.

Recently, hyperspectral remote sensing has gained attention in the remote sensing application community. Hyperspectral imaging generates hundreds of images corresponding to different wavelength channels for the same area on the surface of the earth. A hyperspectral image is a 3-D cube of data with the width and length of the array corresponding to the spatial dimensions and the continuous spectrum of each point as the third dimension, which enables discrimination of materials based on their spectral characteristics. One of the most important applications of hyperspectral data is image classification, where pixels are labeled to one of the classes based on their spectral characteristics.


\section{Related work}
The overall objective of hyperspectral image classification procedures is to automatically categorize all pixels in image into land cover classes (Lu  and Weng, 2007 [5]). Based on the spectral features, several classification algorithms have been used in this field.

\begin{itemize}
\item {\bf Maximum likelihood classification.} \\
Maximum likelihood decision rule is based on Gaussian estimate of the probability density function of each class. Maximum likelihood classifier evaluates both the variance and covariance of the spectral response patterns in classifying an unknown pixel.
\item {\bf Neural networks classifier. } \\
Neural networks (Atli.J et al., 1995 [1]) have been applied successfully in various fields. Neural networks are networks which need a long training time but are relatively fast data classifier. For very high dimensional data, the training time of a neural network can be very long and the resulting neural network can be very complex. This leads to the importance of feature reduction mechanisms for neural networks.
\item {\bf Decision trees. }\\
Decision tree classifier breaks a complex classification problem into multiple stages of simpler decision-making processes (Safavian and Landgrebe, 1991 [10]). Decision trees are trees that classify instances by sorting them based on feature values. Each node in a decision tree represents a feature in an instance to be classified, and each branch represents a value that the node can assume (Murthy,1998 [7]).
\item {\bf Support Vector Machine (SVM).}\\
Specific attention has been dedicated to support vector machines for the classification of remotely sensed images recently (Hermes et al., 1999; Roli and Fumera, 2001; Hung et al., 2002 [4]). The interest in growing Support Vector Machines (Vapnik, 1998; Burges, 1998; http://www.kernal-Machines.org/tutorial.html [11]) is confirmed by their successful implementation in numerous other pattern recognition applications like biomedical applications (El-Naqa et al., 2002 [2]), image compression (Robinson and Kecman,2003 [9]), and three dimensional object recognition (Pontil and Verri, 1998 [8]).
\item {\bf Knowledge based Classifiers.}\\
Different kinds of ancillary data, such as digital elevation model, soil map, housing and temperature are readily available; they may be incorporated into a classification procedure in different ways.
\item {\bf Contextual Classifiers.}\\
In contextual classifiers, the spatially neighboring pixel information is used. Contextual classifiers are developed to cope with the problem of intraclass spectral variations (Gong and Howarth, 1992 [3]). To improve the classification results, it exploits spatial information among neighboring pixels (Magnussen et al., 2004 [6]).
\end{itemize}

\section{Dataset Description}

The Indian Pines dataset was acquired by the NASA Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) sensor over the Indian Pines agricultural site in northwestern Indiana in June 1992, at 20-m spatial resolution and 10-nm spectral resolution over the range of 400-2500 nm. There are 220 spectral bands in the image. Ground reference data were comprised of 10,366 labeled samples assigned to 16 classes.
The crop-tillage related classes relate to the management practices used for tilling in a particular field. For example, Soybeans-no till refers to a field in which soybeans are planted, but no tillage was performed after the last harvest, thereby leaving a significant quantity of crop residue. Soybeans-min till refers to tillage which breaks up the soil, reducing the quantity of remaining residue, while Soybeans-clean till refers to fields that are fully ploughed after the previous harvest, leaving essentially no residue. Corn-tillage related classes are defined similarly. True-color composite and relative ground reference maps are shown in Fig. 1, while the details related to the number of labeled samples is listed in Fig. 2.

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{pic}
\caption{Indian Pines dataset (a) True-color composite image; (b) Ground reference map}
\label{fig:pic}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{table}
\caption{Number of labeled samples for Indian Pines dataset}
\label{fig:table}
\end{figure}


%Below is a pilot evaluation plan:
%\begin{itemize}
%	\item Data Check \\
%	The validity of the whole data set will be checked. Any missing values, non-numeric values and outliers will be identified.
%	\item Data split\\
%	The whole data set will be split into training and testing set. Because the data set is unbalanced (the smallest label group contains 20 examples, while the largest label group contains more than 2000 examples, see \textbf{Figure 2}), up-sampling of small label groups or down-sampling of the large label groups will be conducted when the training data is selected. In addition, the training set will be further randomly divided into 5 folds of equal size, in order to facilitate the following cross validation process.
%	\item Feature Selection \\
%	Considering the dimensionality of the data set is relatively high, Principal component analysis (PCA) will be used to conduct dimensionality reduction. We plan to keep at least 90\% of the original variance, and the principal components will be used in the following data analysis.
%	\item Model Selection \\
%	Considering our task is classification based on numeric features, we plan to construct models using the following algorithms:
%		\begin{itemize}
%		\item Random Forest \\
%		Hyper-parameter to be tuned: Randomly selected features, maxiumn number of trees. This model is available in the Sklearn package of Python.
%		\item Neural Network \\
%		For this model, BackpropTrainer algorithm from Pybrain package of Python will be used. The hyper-parameters include: the hidden layers, learning rate, batch size and so on.
%		\item Linear SVM
%		For this model, the tunable hyper-parameters include cost and maximum iterations. This model is available in the Sklearn package of Python.
%		\item Kernel SVM
%		If necessary, different kernel functions will test. The available kernels in the Sklearn package include: linear, polynominal, rbf, sigmoid and precomputed.
%		\end{itemize}
%		
%	\item Model evaluation \\
%	After the construction of the above models, the performance of the different models will be evaluated by using 5-fold cross validation. Since the correct label of all data points are known, misclassification rate will be used as the subjective function.
%	\item Model testing \\
%	For the models with satisfied performance, the error rate will be tested against the held out testing data.\\
%	
%\end{itemize}


\section{Problem Formulation and Data Preprocessing}

The hyperspectral image classification task is perfectly suitable to be modeled as a machine learning problem. The problem is challenging because the feature space is high dimensional with limited number of labeled data. Furthermore, it is a multi-class classification problem. Each pixel of the hyperspectral image is a sample in this problem. For each pixel, it contains 220 spectral bands, so there are 220 features for each sample. To conduct the classification task, we did the following things to preprocess the data.

\begin{itemize}
  \item Remove noisy spectral bands (features), and use 200 out of 220 as final features.
  \item Ignore minor classes with very labeled samples, and focus on 12 out of 16 classes.
  \item For each class, randomly select 100 labeled samples for training and 100 for testing.
  \item Normalize the data for each feature.
\end{itemize}

\section{Selected Models}

Considering the high dimensionality of the data, we used the following three classifiers:

\begin{itemize}
  \item Kernel SVM: Gaussian kernel, cross-validation for parameter selection
  \item Random Forest: Use cross validation to select the number of trees and the number of features for each split
  \item Adaboost: Use Decision tree as basic classifier and cross-validation for parameter selection
\end{itemize}


   Among the above three models, Kernel SVM implement kernel functions to decrease the bias of the classification model. Random forest is a decision tree based method with bagging resampling technique, while Adaboost is a decision tree based method with boosting resampling technique. Both of them attempt to decrease the bias of the model without sacrificing the variance, in other words, avoid overfitting. 
   
   The above three models were chosen so that the effects of the three popular method to improve the performance of a classification model: kernel function, bagging and boosting could be easily compared under the scope of hyperspectral data classification.
 
 
\section{Results and Conclusion}
A total of 1200 examples are used for model training. For each of the three models, hyper parameters were chosen based on the result of 5-fold cross validation. The result of the hyper parameter selection is shown in Table 1.
\begin{center}
	\begin{tabular}{|l|l|l|l|}
		\hline
		Model	&	Hyper-parameter	&	Trained Values	&	Best Value	\\ \hline
		SVM		&	Kernel type		&	Linear, Polynomial, RBF, sigmoid	& RBF \\
		SVM		&	Weight for regularizer &	0:10 (step=0.5)	&	6.5	\\
		SVM		&	Gamma			&	0:10 (step=0.5)			&	0.5 \\
		RF		&	Number of Trees &	50, 100, 150, 200	&	150		\\ 
		RF		&	Max splitting features & 2:20		&	5			\\
		ADA		&	Learning rate	&	0.1, 0.5, 1, 2	&	1			\\
		ADA		& 	Max depth		&	1:13			&	9			\\
		\hline
	\end{tabular}
\end{center}


To evaluation the performances of the three models, we have calculated the overall accuracies and class specific accuracies on the testing set. The results are shown in the following table.

%\begin{center}
%\captionof{table}{Accuracies on training set}
%\begin{tabular}{|l|c|c|c|}
%  \hline
%  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
%                  & Kernel SVM & Decision Tree & Adaboost \\
%  Overall Accuracy & $\mathbf{0.85}$ & 0.80 & 0.82 \\
%  Corn-no till (C1) & $\mathbf{0.72}$ & 0.68 & 0.71 \\
%  Corn-min till (C2) & $\mathbf{0.67}$ & 0.60 & 0.55 \\
%  Corn (C3)          & $\mathbf{0.91}$ & 0.84 & 0.88 \\
%  Soybeans-no till (C4) & $\mathbf{0.82}$ & 0.77 & 0.76 \\
%  Soybeans-min till (C5) & 0.77 & 0.77 & $\mathbf{0.80}$ \\
%  Soybeans-clean till (C6) & $\mathbf{0.78}$ & 0.66 & 0.70 \\
%  Grass/Pasture (C7) & $\mathbf{0.95}$ & 0.91 & $\mathbf{0.95}$ \\
%  Grass/Trees (C8) & $\mathbf{0.94}$ & 0.91 & $\mathbf{0.94}$ \\
%  Hay-windrowed (C9) & $\mathbf{0.99}$ & $\mathbf{0.99}$ & $\mathbf{0.99}$ \\
%  Wheat (C10) & $\mathbf{0.99}$ & $\mathbf{0.99}$ & $\mathbf{0.99}$ \\
%  Woods (C11) & 0.83 & $\mathbf{0.84}$ & 0.82 \\
%  Building-Grass-Trees-Drive (C12) & $\mathbf{0.78}$ & 0.65 & $\mathbf{0.78}$ \\
%  \hline
%\end{tabular}
%\end{center}

\begin{center}
\captionof{table}{Accuracies on testing set}
\begin{tabular}{|l|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
                  & Kernel SVM & Random Forest & Adaboost \\ \hline
  Overall Accuracy & $\mathbf{0.85}$ & 0.80 & 0.82 \\
  Corn-no till (C1) & $\mathbf{0.72}$ & 0.68 & 0.71 \\
  Corn-min till (C2) & $\mathbf{0.67}$ & 0.60 & 0.55 \\
  Corn (C3)          & $\mathbf{0.91}$ & 0.84 & 0.88 \\
  Soybeans-no till (C4) & $\mathbf{0.82}$ & 0.77 & 0.76 \\
  Soybeans-min till (C5) & 0.77 & 0.77 & $\mathbf{0.80}$ \\
  Soybeans-clean till (C6) & $\mathbf{0.78}$ & 0.66 & 0.70 \\
  Grass/Pasture (C7) & $\mathbf{0.95}$ & 0.91 & $\mathbf{0.95}$ \\
  Grass/Trees (C8) & $\mathbf{0.94}$ & 0.91 & $\mathbf{0.94}$ \\
  Hay-windrowed (C9) & $\mathbf{0.99}$ & $\mathbf{0.99}$ & $\mathbf{0.99}$ \\
  Wheat (C10) & $\mathbf{0.99}$ & $\mathbf{0.99}$ & $\mathbf{0.99}$ \\
  Woods (C11) & 0.83 & $\mathbf{0.84}$ & 0.82 \\
  Building-Grass-Trees-Drive (C12) & $\mathbf{0.78}$ & 0.65 & $\mathbf{0.78}$ \\
  \hline
\end{tabular}
\end{center}

From the above results, we can see that kernel SVM performs the best among the three approaches. This is because SVM with Gaussian kernel project the original data to a much higher feature space in which the data are linearly separable. Using the cross-validation allow us to select such a feature space. For the other two ensemble approaches, boosting is a little better than bagging for this data. 


\section*{References}
[1] Atli,J. Benediktsson, Johan Nes R, Arnason,S.K (1995),Classification and feature extraction of AVIRIS data, IEEE Trans. On Geo Science and Remote Sensing, 33,

[2] El-Naqa.I , Yongi Y, Wernick M.N, Galatsanos .P.N, Nishikawa.M.R ,(2002), A support vector machine approach for detection of micro calcifications, IEEE Transactions on medical Image,21,12, 1552-1563.

[3] Gong.P and Howaeth P.J, (1992), Frequency-based contextual classification and gray level vector reduction for land use identification, Photogrammetric Engineering and Remote sensing, 58, 423-437.

[4] Hermes .L, D.Frieauff, J.Puzicha and J.M Buhmann, (1999). Support Vector Machines for land usage classification in land set TM imagery, in Proc. IGARSS, Hamburg, Germany, 348-350

[5] Lu .D and Weng.Q, (2007), A Survey of Image Classification methods and techniques for improving classification Performance, International Journal of Remote Sensing, 28, 5, 823-870.

[6] Magnussen.S, Boudewyn.P and Wulder.M, (2004), Contextual Classification of Landsat TM images to forest inventory cover types. International Journal of Remote Sensing. 24 2421-2440

[7] Murthy,(1998), Automatic Construction of Decision Trees from Data. A Multi-Disciplinary Survey, Data Mining and Knowledge, 345-389

[8] Pontil.M, Verri.A (1998), Support Vector Machines for 3D Object Recognition, IEEE Transaction pattern Anal. Machine Intel, 20, 637-646.

[9] Robinson, Kecman.V, (2003),Combining Support Vector Machine learning with the discrete cosine transform in Image Compression, IEEE Trans. Neural Networks, 14, 950-958.

[10] Safavian.S.R and D. Landgrebe, (1991), A Survey of decision tree Classifier Methodology, IEEE Transaction on Systems, Man and Cybernetics ,660-674.

[11] Vapnik. V.N, (1998), Statistical Learning Theory, Newyork, Wiley.

%\nocite{*}
%\bibliographystyle{plainnat}
%\bibliography{all}

\end{document}
